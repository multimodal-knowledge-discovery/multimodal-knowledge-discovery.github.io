<!DOCTYPE html>
<html lang="en-US">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta charset="UTF-8">

<meta property="og:image" content="favicon.png">

<link rel="icon" href="favicon.png" type="image/png">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Online Multimodal Knowledge Discovery</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="Welcome">
<meta property="og:locale" content="en_US">
<meta name="description" content="Online Multimodal Knowledge Discovery at ICDM 2020: 20th IEEE International Conference on Data Mining.">
<meta property="og:description" content="Online Multimodal Knowledge Discovery at ICDM 2020: 20th IEEE International Conference on Data Mining.">
<link rel="canonical" href="https://multimodal-knowledge-discovery.github.io//">
<meta property="og:url" content="https://multimodal-knowledge-discovery.github.io//">
<meta property="og:site_name" content="Online Multimodal Knowledge Discovery">
<script type="application/ld+json">
{"description":"Online Multimodal Knowledge Discovery at ICDM 2020: 20th IEEE International Conference on Data Mining.","name":"Online Multimodal Knowledge Discovery","@type":"WebSite","url":"https://multimodal-knowledge-discovery.github.io//","headline":"Welcome","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=c64178984bc3b4827ae20ab3ebe9e05d036cc516">
  </head>
  <body>

    <header class="page-header" role="banner">
    <h1>Online Multimodal Knowledge Discovery</h1>
    <h2>ICDM 2020: 20th IEEE International Conference on Data Mining</h2>
    </header>

    <main id="content" class="main-content" role="main">


<div style="display: flex, margin: auto" align="center">  
  <div style="width:64px;" align="center">
    <img src="favicon.png" style="max-width:100%;">
  </div>
  </div>

      <h2 id="welcome">Welcome</h2>

<p>New social technologies and widespread access to the internet have allowed for new forms of content creation, connectivity and information sharing. With vast unstructured data and limited labels, organizing and reconciling information from different sources and modalities with bounded supervision is one of the current challenges in machine learning. This tutorial focuses on using multimodal representations for graph-regularized or semi-supervised learning, and uses as case study two real-world multi-domain datasets which prompt for understanding the fine-grained visual and linguistic semantics.</p>

<h2 id="venue">Venue</h2>

<p>The <em>Online Multimodal Knowledge Discovery</em> tutorial will be held virtually at <a href="http://icdm2020.bigke.org/"><em>ICDM 2020: 20th IEEE International Conference on Data Mining</em></a> on November 18th, 2020, from 14:30 to 16:30 CET.</p>

<h2 id="outline">Outline</h2>

<table>
  <tbody>
    <tr>
      <td>Section</td>
      <td>Subsection</td>
      <td style="text-align:right;">min</td>
    </tr>
    <tr>
      <td rowspan="2">Introduction
<br>
</td>
      <td>The landscape of online content</td>
      <td style="text-align:right;">10</td>
    </tr>
    <tr>
      
      <td>A case for multimodal knowledge reconciliation</td>
      <td style="text-align:right;">5</td>
    </tr>
    <tr>
      <td rowspan="3">Natural
<br>
Language
<br>
Processing</td>
      <td>From word embeddings to contextualized representations</td>
      <td style="text-align:right;">10</td>
    </tr>
    <tr>
      
      <td>Fine-tuning pretrained models on downstream tasks</td>
      <td style="text-align:right;">5</td>
    </tr>
    <tr>
      
      <td>The textual entailment problem</td>
      <td style="text-align:right;">5</td>
    </tr>
    <tr>
      <td rowspan="2">Structured Data
<br>
</td>
      <td>Semi-structured and tabular text</td>
      <td style="text-align:right;">5</td>
    </tr>
    <tr>
      
      <td>Knowledge graphs</td>
      <td style="text-align:right;">5</td>
    </tr>
    <tr>
      <td>Neural Graph Learning</td>
      <td>Leveraging structured signals with Neural Structured Learning</td>
      <td style="text-align:right;">10</td>
    </tr>
    <tr>
      <td>Break</td>
      <td>-</td>
      <td style="text-align:right;">5</td>
    </tr>
    <tr>
      <td rowspan="3">Multimodal Learning
<br><br>
</td>
      <td>Learning joint representations for visual and language tasks</td>
      <td style="text-align:right;">20</td>
    </tr>
    <tr>
      
      <td>Self-Supervised Multimodal Versatile Networks</td>
      <td style="text-align:right;">20</td>
    </tr>
    <tr>
      
      <td>Multimodal representations for knowledge reconciliation</td>
      <td style="text-align:right;">10</td>
    </tr>
    <tr>
      <td rowspan="2">Final considerations
<br>
</td>
      <td>Closing notes</td>
      <td style="text-align:right;">5</td>
    </tr>
    <tr>
      
      <td>Q&amp;A</td>
      <td style="text-align:right;">5</td>
    </tr>
    <tr>
      <td>Total</td>
      <td>â€“</td>
      <td style="text-align:right;">120</td>
    </tr>
  </tbody>
</table>

<h2 id="slides">Slides</h2>

<iframe
  src="https://docs.google.com/presentation/d/1iuOnstXckRfuYi_2r_8KI3xzlUMdOoK8JmWALVmhuoo/embed?usp=sharing&resourcekey=0-iL774GeWz-c2O3ObcL5bTg"
  frameborder="0"
  width="800"
  height="600"
></iframe>

<h2 id="reading-list">Reading list</h2>

<h3 id="natural-language-processing">Natural Language Processing</h3>

<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>, Vaswani et at., 2017.</li>
<li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, Devlin et at., 2018.</li>
<li><a href="https://arxiv.org/abs/1909.11942">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a>, Lan et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1706.03762">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>, Raffel et at., 2019.</li>
<li><a href="https://github.com/tensorflow/workshops/tree/master/kdd2019">Deep Learning for NLP with Tensorflow</a>, Ilharco et at., 2019.</li>
<li><a href="http://gabrielilharco.com/publications/EMNLP_2020_Tutorial__High_Performance_NLP.pdf">High Performance Natural Language Processing</a>, Ilharco et at., 2020.</li>
</ul>

<h3 id="neural-graph-learning">Textual Entailment</h3>

<ul>
<li><a href="https://allenai.org/content/team/peterc/publications/RTE7_overview.proceedings.pdf">The Seventh PASCAL Recognizing Textual Entailment Challenge</a>, Bentivogli et at., 2011.</li>
<li><a href="https://www.mitpressjournals.org/doi/full/10.1162/COLI_a_00097">Did It Happen? The Pragmatic Complexity of Veridicality Assessment</a>, de Marneffe et at., 2012.</li>
<li><a href="https://cims.nyu.edu/~sbowman/multinli/">The Multi-Genre Natural Language Inference (MultiNLI) corpus</a>, Williams et at., 2017.</li>
<li><a href="https://arxiv.org/abs/1809.05053">XNLI: Evaluating Cross-lingual Sentence Representations</a>, Conneau et at., 2018.</li>
</ul>

<h3 id="neural-graph-learning">Structured Data</h3>

<ul>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3329781.3332266">Industry-scale Knowledge Graphs</a>, Noy et at., 2019.</li>
<li><a href="https://www.meetup.com/NLP-Zurich/events/263974607/">Understanding categorical semantic compatibility in KG</a>, Muxagata et at., 2019.</li>
</ul>


<h3 id="neural-graph-learning">Neural Graph Learning</h3>

<ul>
<li><a href="https://arxiv.org/abs/1703.04818">Neural Graph Machines: Learning Neural Networks Using Graphs</a>, Bui et at., 2017.</li>
<li><a href="https://github.com/tensorflow/neural-structured-learning/tree/master/workshops/kdd_2020">Neural Structured Learning: Training Neural Networks with Structured Signals</a>, Heydon et at., 2020.</li>
</ul>

<h3 id="multimodal-learning">Multimodal Learning</h3>

<ul>
<li><a href="https://people.csail.mit.edu/khosla/papers/icml2011_ngiam.pdf">Multimodal Deep Learning</a>, Ngiam et at., 2011.</li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf">DeViSE: A Deep Visual-Semantic Embedding Model</a>, Frome et at., 2013.</li>
<li><a href="https://arxiv.org/abs/1804.02516">Learning a Text-Video Embedding from Incomplete and Heterogeneous Data</a>, Miech et at., 2018.</li>
<li><a href="https://arxiv.org/abs/1904.01766">VideoBERT: A Joint Model for Video and Language Representation Learning</a>, Sun et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1906.03327">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</a>, Miech et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1906.05743">Learning Video Representations using Contrastive Bidirectional Transformer</a>, Sun et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1907.13487">Use What You Have: Video Retrieval Using Representations From Collaborative Experts</a>, Liu et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1908.07490">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</a>, Tan et at., 2019.</li>
<li><a href="https://arxiv.org/abs/1908.08530">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a>, Su et at., 2020.</li>
<li><a href="https://arxiv.org/abs/1912.02315">12-in-1: Multi-Task Vision and Language Representation Learning</a>, Lu et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2003.13594">Speech2Action: Cross-modal Supervision for Action Recognition</a>, Nagrani et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2006.16228">Self-Supervised MultiModal Versatile Networks</a>, Alayrac et at., 2020.</li>
<li><a href="https://arxiv.org/abs/2007.10639">Multi-modal Transformer for Video Retrieval</a>, Gabeur et at., 2020.</li>
</ul>

<h3 id="multimodal-learning">Datasets</h3>

<ul>
<li><a href="https://figshare.com/articles/PHEME_dataset_for_Rumour_Detection_and_Veracity_Classification/6392078/1">PHEME dataset for Rumour Detection and Veracity Classification</a>, Kochkina et at., 2018.</li>
<li><a href="https://www.aclweb.org/anthology/D19-1475/">MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims</a>, Augenstein et at., 2019.</li>
</ul>


<h2 id="tutors">Tutors</h2>

<div style="display: flex">
  <div style="width:250px;" align="center">
    <a href="https://www.linkedin.com/in/cesarilharco/">
    <img alt="Cesar Ilharco" src="./images/cesar_ilharco.jpg" style="max-width:100%;">
    </a><br>
    <a href="https://www.linkedin.com/in/cesarilharco/">Cesar Ilharco</a><br>
    Senior Research Engineer,<br>Google Research
  </div>
  
  <div style="width:2%">
  </div>
  
  <div style="width:250px;" align="center">
    <a href="https://scholar.google.com/citations?user=uK4Bcf0AAAAJ">
    <img alt="Ricardo Marino" src="./images/ricardo_marino.jpg" style="max-width:100%;">
    </a><br>
  <a href="https://scholar.google.com/citations?user=uK4Bcf0AAAAJ">Ricardo Marino</a><br>
    Data Scientist,<br>Google Research
  </div>

  <div style="width:2%">
  </div>

  <div style="width:250px;" align="center">
    <a href="http://bulian.org/">
    <img alt="Jannis Bulian" src="./images/jannis_bulian.jpg" style="max-width:100%;">
    </a><br>
  <a href="http://bulian.org/">Jannis Bulian</a><br>
    Senior Software Engineer,<br>Google Research
  </div>
</div>

<br>

<div style="display: flex">  
  <div style="width:250px;" align="center">
    <a href="http://www.robots.ox.ac.uk/~arsha/">
    <img alt="Arsha Nagraniâ€Ž" src="./images/arsha_nagrani.jpg" style="max-width:100%;">
    </a><br>
  <a href="http://www.robots.ox.ac.uk/~arsha/">Arsha Nagraniâ€Ž</a><br>
    Research Scientist,<br>Google Research
  </div>

  <div style="width:2%">
  </div>

  <div style="width:250px;" align="center">
    <a href="https://scholar.google.com/citations?user=yN7CEicAAAAJ">
    <img alt="Lucas Smaira" src="./images/lucas_smaira.jpg" style="max-width:100%;">
    </a><br>
  <a href="https://scholar.google.com/citations?user=yN7CEicAAAAJ">Lucas Smaira</a><br>
    Senior Research Engineer,<br>DeepMind 
  </div>
        
  <div style="width:2%">
  </div>

  <div style="width:250px;" align="center">
    <img alt="Afsaneh Shirazi" src="./images/afsaneh_shirazi.jpg" style="max-width:100%;">
    <br>
  Afsaneh Shirazi<br>
  Senior Staff Software Engineer,<br>Google Research
  </div>
</div>


<h2 id="tutors">Acknowledgements</h2>

We would like to thank Gabriel Ilharco, Abe Ittycheriah, Thomas Leung, Felipe Ferreira, Mor Naaman, Isabelle Augenstein, Arkaitz Zubiaga, Elena Kochkina, Arjun Gopalan, Da-Cheng Juan, Jordan Boyd-Graber, Chen Sun, Cong Yu, Tania Bedrax-Weiss, Cordelia Schmid, Chris Breglerâ€Ž and Rahul Sukthankar.

      
    </main>
  </body>
</html>
